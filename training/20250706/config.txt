[model]
block_size = 1024
vocab_size = 32768
n_layer = 14
n_head = 16
n_embd = 896
dropout = 0.0
context_length = 1024

[hypers]
learning_rate = 6e-4
nb_epochs = 3
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
min_lr = 6e-5

[files]
checkpoint_dir = ./checkpoints/
log_dir = ./logs/
log_file = training.log
stat_file = stats.csv
data_dir = ./data/
tokenizer_dir = ./fr_mixed_tokenizer/

[training]
# tokens processed per iteration = gradient_accumulation_steps * batch_size * block_size
gradient_accumulation_steps = 8 
batch_size = 64    
block_size = 1024 
micro_batch_size = 64 # no longer used
log_interval = 10
checkpoint_interval = 1000
eval_interval = 200 
validation_steps = 20
gen_text_interval = 128
num_generation_sequences = 2
n_checkpoint_files = 4
tokens_per_shard = 1048577
n_shards = 1835
compile = True
max_steps = 1000000
nb_dataset_tokens = 1.8e9
eval_only = False
# resume or scratch
init_from = resume
