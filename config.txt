[model]
block_size = 1024
vocab_size = 32768
n_layer = 14
n_head = 16
n_embd = 896
dropout = 0.1
context_length = 1024

[hypers]
lr = 6e-4
epochs = 3

[files]
checkpoint_dir = ./checkpoints/
log_dir = ./logs/
log_file = training.log
stat_file = stats.csv
token_dir = ./data/shards/
tokenizer_dir = ./fr_mixed_tokenizer/

[training]
batch_size = 65536
micro_batch_size = 64
log_interval = 16
checkpoint_interval = 1000
eval_interval = 64
validation_steps = 4
gen_text_interval = 128
num_generation_sequences = 2
n_checkpoint_files = 4
tokens_per_shard = 1048577
n_shards = 1835
use_compile = True
max_steps = 1000000
