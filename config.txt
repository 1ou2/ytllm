[model]
block_size = 1024
vocab_size = 32768
n_layer = 12
n_head = 12
n_embd = 768
context_length = 1024

[hypers]
lr = 6e-4
epochs = 3

[files]
checkpoint_dir = ./checkpoints/
log_dir = ./logs/
log_file = training.log
stat_file = stats.csv
token_dir = ./data/shards/
tokenizer_dir = ./fr_mixed_tokenizer/

[training]
batch_size = 32768
micro_batch_size = 2
log_interval = 5
checkpoint_interval = 10
eval_interval = 4
validation_steps = 5
gen_text_interval = 3
num_generation_sequences = 3
n_checkpoint_files = 3
tokens_per_shard = 1048577
n_shards = 1835
use_compile = False
max_steps = 100000
