[model]
block_size = 1024
vocab_size = 32768
n_layer = 12
n_head = 12
n_embd = 768
context_length = 1024

[hypers]
lr = 6e-4
epochs = 3

[files]
checkpoint_dir = ./checkpoints/
log_dir = ./logs/
log_file = training.log
stat_file = stats.csv
token_dir = ./data/tokens/
tokenizer_dir = ./data/tokenizer/

[training]
batch_size = 32
micro_batch_size = 4
log_interval = 10
checkpoint_interval = 500
eval_interval = 1000
validation_steps = 50
gen_text_interval = 2000
num_generation_sequences = 3
n_checkpoint_files = 3
tokens_per_shard = 1048577
n_shards = 10
use_compile = False
max_steps = 100000
